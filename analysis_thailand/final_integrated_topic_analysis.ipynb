 {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Integrated Topic Analysis with DTM Metrics\n",
    "\n",
    "This notebook provides a complete, integrated analysis of your topic models. It calculates all Dynamic Topic Model (DTM) metrics from scratch and combines them with comprehensive topic information, including frequency, temporal patterns, and word analysis.\n",
    "\n",
    "## 🎯 Key Features:\n",
    "\n",
    "1.  **DTM Metrics from Scratch**: Calculates TTC, TTS, TTQ, TC, TD, and DTQ.\n",
    "2.  **Integrated Topic Profiles**: For **every topic**, you get:\n",
    "    *   Frequency and persistence statistics.\n",
    "    *   Temporal patterns (duration, start/end dates).\n",
    "    *   Word analysis (top words, diversity).\n",
    "    *   **Calculated DTM quality scores (e.g., TTQ).**\n",
    "3.  **Comprehensive Visualizations**:\n",
    "    *   Datetime axes for all temporal plots.\n",
    "    *   Charts comparing topics by both frequency and DTM quality.\n",
    "    *   Cross-platform comparison between YouTube and Telegram.\n",
    "4.  **Unified Analysis**: A single, powerful view to understand topic characteristics and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Ready for final integrated analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw topic data\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load and preprocess raw topic data from both platforms\"\"\"\n",
    "    \n",
    "    print(\"Loading raw topic data...\")\n",
    "    \n",
    "    # Load YouTube and Telegram topic data\n",
    "    youtube_topics = pd.read_csv('analysis_thailand/bert_overtime/youtube/youtube_topics_over_time_raw.csv')\n",
    "    telegram_topics = pd.read_csv('analysis_thailand/bert_overtime/telegram/telegram_topics_over_time_raw.csv')\n",
    "    \n",
    "    # Preprocess function\n",
    "    def preprocess_dataset(df, source_name):\n",
    "        \"\"\"Preprocess individual dataset\"\"\"\n",
    "        df = df.copy()\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "        df['Source'] = source_name\n",
    "        df['Year'] = df['Timestamp'].dt.year\n",
    "        df['Month'] = df['Timestamp'].dt.month\n",
    "        df['Year_Month'] = df['Timestamp'].dt.to_period('M')\n",
    "        return df\n",
    "    \n",
    "    # Process both datasets\n",
    "    youtube_topics = preprocess_dataset(youtube_topics, 'YouTube')\n",
    "    telegram_topics = preprocess_dataset(telegram_topics, 'Telegram')\n",
    "    \n",
    "    print(f\"\\n=== Raw Data Summary ===\")\n",
    "    print(f\"YouTube: {len(youtube_topics)} records, {len(youtube_topics['Topic'].unique())} unique topics\")\n",
    "    print(f\"Telegram: {len(telegram_topics)} records, {len(telegram_topics['Topic'].unique())} unique topics\")\n",
    "    print(f\"YouTube date range: {youtube_topics['Timestamp'].min()} to {youtube_topics['Timestamp'].max()}\")\n",
    "    print(f\"Telegram date range: {telegram_topics['Timestamp'].min()} to {telegram_topics['Timestamp'].max()}\")\n",
    "    \n",
    "    return youtube_topics, telegram_topics\n",
    "\n",
    "# Load the data\n",
    "youtube_data, telegram_data = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DTM Metrics Calculator from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTMMetricsCalculator:\n",
    "    \"\"\"\n",
    "    Calculate Dynamic Topic Model metrics from scratch using raw topic data.\n",
    "    Implements TTC, TTS, TTQ, TC, TD, TQ, DTQ metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, topics_df, dataset_name, top_n_topics=30, n_words=10):\n",
    "        self.topics_df = topics_df\n",
    "        self.dataset_name = dataset_name\n",
    "        self.top_n_topics = top_n_topics\n",
    "        self.n_words = n_words\n",
    "        print(f\"\\n=== Initializing DTM Metrics Calculator for {dataset_name} ===\")\n",
    "        self._prepare_data()\n",
    "        \n",
    "    def _prepare_data(self):\n",
    "        \"\"\"Prepare data for DTM evaluation\"\"\"\n",
    "        valid_topics = self.topics_df[self.topics_df['Topic'] != -1].copy()\n",
    "        if valid_topics.empty:\n",
    "            raise ValueError(\"No valid topics found in the data\")\n",
    "        topic_counts = valid_topics.groupby('Topic')['Frequency'].sum().sort_values(ascending=False)\n",
    "        self.selected_topics = topic_counts.head(self.top_n_topics).index.tolist()\n",
    "        self.filtered_df = valid_topics[valid_topics['Topic'].isin(self.selected_topics)].copy()\n",
    "        self.timestamps = sorted(self.topics_df['Timestamp'].unique())\n",
    "        self.num_time_slices = len(self.timestamps)\n",
    "        self.timestamp_map = {ts: i for i, ts in enumerate(self.timestamps)}\n",
    "        self._prepare_topic_words_by_time()\n",
    "        self._prepare_corpus()\n",
    "    \n",
    "    def _prepare_topic_words_by_time(self):\n",
    "        \"\"\"Prepare topic words organized by time and topic\"\"\"\n",
    "        self.topic_words_by_time = {}\n",
    "        for t_idx, timestamp in enumerate(self.timestamps):\n",
    "            self.topic_words_by_time[t_idx] = {}\n",
    "            for topic_id in self.selected_topics:\n",
    "                mask = (self.topics_df['Timestamp'] == timestamp) & (self.topics_df['Topic'] == topic_id)\n",
    "                topic_data = self.topics_df[mask]\n",
    "                if not topic_data.empty:\n",
    "                    words = topic_data['Words'].iloc[0]\n",
    "                    if isinstance(words, str):\n",
    "                        word_list = [word.strip() for word in words.split(',') if word.strip()]\n",
    "                        self.topic_words_by_time[t_idx][topic_id] = word_list[:self.n_words]\n",
    "                    else:\n",
    "                        self.topic_words_by_time[t_idx][topic_id] = []\n",
    "                else:\n",
    "                    self.topic_words_by_time[t_idx][topic_id] = []\n",
    "\n",
    "    def _prepare_corpus(self):\n",
    "        \"\"\"Prepare reference corpus for coherence calculations\"\"\"\n",
    "        all_words = [word for time_dict in self.topic_words_by_time.values() for topic_words in time_dict.values() for word in topic_words]\n",
    "        self.vocabulary = list(set(all_words))\n",
    "        self.corpus_texts = [' '.join(topic_words) for time_dict in self.topic_words_by_time.values() for topic_words in time_dict.values() if topic_words]\n",
    "        self.tokenized_texts = [text.split() for text in self.corpus_texts]\n",
    "        doc_freq = {word: set() for word in self.vocabulary}\n",
    "        for i, text in enumerate(self.tokenized_texts):\n",
    "            for word in set(text):\n",
    "                if word in doc_freq:\n",
    "                    doc_freq[word].add(i)\n",
    "        self.doc_freq = doc_freq\n",
    "\n",
    "    def _calculate_pmi_coherence(self, word_pairs):\n",
    "        \"\"\"Calculate PMI-based coherence for word pairs\"\"\"\n",
    "        if not word_pairs: return 0.0\n",
    "        coherence_sum = 0.0\n",
    "        pair_count = 0\n",
    "        epsilon = 1e-10\n",
    "        num_docs = len(self.tokenized_texts)\n",
    "        for word_i, word_j in word_pairs:\n",
    "            if word_i in self.vocabulary and word_j in self.vocabulary:\n",
    "                docs_i = self.doc_freq[word_i]\n",
    "                docs_j = self.doc_freq[word_j]\n",
    "                count_ij = len(docs_i.intersection(docs_j))\n",
    "                if count_ij > 0:\n",
    "                    prob_i = len(docs_i) / num_docs\n",
    "                    prob_j = len(docs_j) / num_docs\n",
    "                    prob_ij = count_ij / num_docs\n",
    "                    coherence = np.log((prob_ij + epsilon) / (prob_i * prob_j + epsilon))\n",
    "                    coherence_sum += coherence\n",
    "                    pair_count += 1\n",
    "        return coherence_sum / pair_count if pair_count > 0 else 0.0\n",
    "\n",
    "    def calculate_temporal_topic_coherence(self, topic_k, time_t):\n",
    "        if time_t + 1 >= self.num_time_slices: return 0.0\n",
    "        words_t = self.topic_words_by_time[time_t].get(topic_k, [])\n",
    "        words_t_plus = self.topic_words_by_time[time_t + 1].get(topic_k, [])\n",
    "        if not words_t or not words_t_plus: return 0.0\n",
    "        word_pairs = list(itertools.product(words_t, words_t_plus))\n",
    "        return self._calculate_pmi_coherence(word_pairs)\n",
    "\n",
    "    def calculate_temporal_topic_smoothness(self, topic_k, time_t):\n",
    "        if time_t + 1 >= self.num_time_slices: return 0.0\n",
    "        words_i = set(self.topic_words_by_time[time_t].get(topic_k, []))\n",
    "        words_j = set(self.topic_words_by_time[time_t + 1].get(topic_k, []))\n",
    "        if not words_i or not words_j: return 0.0\n",
    "        intersection = len(words_i & words_j)\n",
    "        union = len(words_i | words_j)\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "\n",
    "    def calculate_topic_coherence(self, topic_words):\n",
    "        if not topic_words or len(topic_words) < 2: return 0.0\n",
    "        word_pairs = list(itertools.combinations(topic_words, 2))\n",
    "        return self._calculate_pmi_coherence(word_pairs)\n",
    "\n",
    "    def calculate_topic_diversity(self, all_topic_words):\n",
    "        if not all_topic_words: return 0.0\n",
    "        all_words = [word for sublist in all_topic_words for word in sublist]\n",
    "        if not all_words: return 0.0\n",
    "        unique_words = len(set(all_words))\n",
    "        total_words = len(all_words)\n",
    "        return unique_words / total_words if total_words > 0 else 0.0\n",
    "\n",
    "    def compute_all_metrics(self):\n",
    "        print(f\"\\n=== Computing DTM Metrics for {self.dataset_name} ===\")\n",
    "        results = {'ttc_per_topic_per_time': {}, 'tts_per_topic_per_time': {}, 'ttq_per_topic': {}, 'tc_per_time': {}, 'td_per_time': {}, 'tq_per_time': {}, 'overall_metrics': {}}\n",
    "        for topic_k in self.selected_topics:\n",
    "            results['ttc_per_topic_per_time'][topic_k] = {}\n",
    "            results['tts_per_topic_per_time'][topic_k] = {}\n",
    "            ttc_scores, tts_scores = [], []\n",
    "            for time_t in range(self.num_time_slices - 1):\n",
    "                ttc = self.calculate_temporal_topic_coherence(topic_k, time_t)\n",
    "                tts = self.calculate_temporal_topic_smoothness(topic_k, time_t)\n",
    "                results['ttc_per_topic_per_time'][topic_k][time_t] = ttc\n",
    "                results['tts_per_topic_per_time'][topic_k][time_t] = tts\n",
    "                ttc_scores.append(ttc)\n",
    "                tts_scores.append(tts)\n",
    "            ttq_scores = [c * s for c, s in zip(ttc_scores, tts_scores)]\n",
    "            results['ttq_per_topic'][topic_k] = np.mean(ttq_scores) if ttq_scores else 0.0\n",
    "        for time_t in range(self.num_time_slices):\n",
    "            all_topic_words_t, tc_scores_t = [], []\n",
    "            for topic_k in self.selected_topics:\n",
    "                topic_words = self.topic_words_by_time[time_t].get(topic_k, [])\n",
    "                if topic_words:\n",
    "                    all_topic_words_t.append(topic_words)\n",
    "                    tc_scores_t.append(self.calculate_topic_coherence(topic_words))\n",
    "            results['tc_per_time'][time_t] = np.mean(tc_scores_t) if tc_scores_t else 0.0\n",
    "            results['td_per_time'][time_t] = self.calculate_topic_diversity(all_topic_words_t)\n",
    "            results['tq_per_time'][time_t] = results['tc_per_time'][time_t] * results['td_per_time'][time_t]\n",
    "        all_ttc = [v for scores in results['ttc_per_topic_per_time'].values() for v in scores.values()]\n",
    "        results['overall_metrics']['TTC'] = np.mean(all_ttc) if all_ttc else 0.0\n",
    "        all_tts = [v for scores in results['tts_per_topic_per_time'].values() for v in scores.values()]\n",
    "        results['overall_metrics']['TTS'] = np.mean(all_tts) if all_tts else 0.0\n",
    "        results['overall_metrics']['TTQ'] = np.mean(list(results['ttq_per_topic'].values())) if results['ttq_per_topic'] else 0.0\n",
    "        results['overall_metrics']['TC'] = np.mean(list(results['tc_per_time'].values())) if results['tc_per_time'] else 0.0\n",
    "        results['overall_metrics']['TD'] = np.mean(list(results['td_per_time'].values())) if results['td_per_time'] else 0.0\n",
    "        results['overall_metrics']['TQ'] = np.mean(list(results['tq_per_time'].values())) if results['tq_per_time'] else 0.0\n",
    "        results['overall_metrics']['DTQ'] = 0.5 * (results['overall_metrics']['TQ'] + results['overall_metrics']['TTQ'])\n",
    "        print(f\"✅ All metrics calculated for {self.dataset_name}!\")\n",
    "        self.results = results\n",
    "        return results\n",
    "\n",
    "    def get_metrics_summary(self):\n",
    "        if not hasattr(self, 'results'): raise ValueError(\"Run compute_all_metrics() first.\")\n",
    "        return pd.DataFrame([{'Metric': m, 'Score': s} for m, s in self.results['overall_metrics'].items()])\n",
    "\n",
    "print(\"DTM Metrics Calculator class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate and Visualize DTM Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DTM metrics calculators for both datasets\n",
    "print(\"Creating DTM metrics calculators...\")\n",
    "\n",
    "# Initialize calculators\n",
    "youtube_calculator = DTMMetricsCalculator(\n",
    "    topics_df=youtube_data, \n",
    "    dataset_name=\"YouTube\", \n",
    "    top_n_topics=30, \n",
    "    n_words=10\n",
    ")\n",
    "\n",
    "telegram_calculator = DTMMetricsCalculator(\n",
    "    topics_df=telegram_data, \n",
    "    dataset_name=\"Telegram\", \n",
    "    top_n_topics=30, \n",
    "    n_words=10\n",
    ")\n",
    "\n",
    "# Calculate metrics for both datasets\n",
    "youtube_results = youtube_calculator.compute_all_metrics()\n",
    "telegram_results = telegram_calculator.compute_all_metrics()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🚀 DTM METRICS CALCULATION COMPLETED!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metrics summary\n",
    "def create_metrics_visualization(youtube_calc, telegram_calc):\n",
    "    youtube_summary = youtube_calc.get_metrics_summary()\n",
    "    telegram_summary = telegram_calc.get_metrics_summary()\n",
    "    \n",
    "    # Create comprehensive comparison\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Metric': youtube_summary['Metric'],\n",
    "        'YouTube': youtube_summary['Score'],\n",
    "        'Telegram': telegram_summary['Score']\n",
    "    })\n",
    "    print(\"\\n🔍 COMPREHENSIVE METRICS COMPARISON:\")\n",
    "    print(comparison_df.round(4))\n",
    "    \n",
    "    # Create comparison plot\n",
    "    fig = make_subplots(rows=1, cols=2, specs=[[{'type': 'bar'}, {'type': 'polar'}]], subplot_titles=('DTM Metrics Comparison', 'DTM Metrics Radar Chart'))\n",
    "\n",
    "    fig.add_trace(go.Bar(name='YouTube', x=youtube_summary['Metric'], y=youtube_summary['Score'], marker_color='red'), row=1, col=1)\n",
    "    fig.add_trace(go.Bar(name='Telegram', x=telegram_summary['Metric'], y=telegram_summary['Score'], marker_color='blue'), row=1, col=1)\n",
    "\n",
    "    fig.add_trace(go.Scatterpolar(r=youtube_summary['Score'], theta=youtube_summary['Metric'], fill='toself', name='YouTube', marker_color='red'), row=1, col=2)\n",
    "    fig.add_trace(go.Scatterpolar(r=telegram_summary['Score'], theta=telegram_summary['Metric'], fill='toself', name='Telegram', marker_color='blue'), row=1, col=2)\n",
    "\n",
    "    fig.update_layout(height=500, width=1000, title_text=\"DTM Metrics: YouTube vs Telegram\")\n",
    "    fig.show()\n",
    "\n",
    "create_metrics_visualization(youtube_calculator, telegram_calculator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Integrated Topic Analysis (Frequency, Words, and DTM Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_topics_integrated_overview(data, calculator):\n",
    "    \"\"\"Get a comprehensive overview of all topics, integrating DTM metrics.\"\"\"\n",
    "    dataset_name = calculator.dataset_name\n",
    "    print(f\"\\n📊 INTEGRATED TOPIC OVERVIEW - {dataset_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    valid_data = data[data['Topic'] != -1].copy()\n",
    "    if valid_data.empty: return None\n",
    "    \n",
    "    # Basic frequency and temporal stats\n",
    "    topic_stats = valid_data.groupby('Topic').agg(\n",
    "        Frequency_sum=('Frequency', 'sum'),\n",
    "        Frequency_mean=('Frequency', 'mean'),\n",
    "        Time_Periods=('Timestamp', 'nunique'),\n",
    "        Start_Date=('Timestamp', 'min'),\n",
    "        End_Date=('Timestamp', 'max')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Get DTM metrics for each topic\n",
    "    dtm_metrics = pd.DataFrame.from_dict(calculator.results['ttq_per_topic'], orient='index', columns=['TTQ_Score'])\n",
    "    dtm_metrics.index.name = 'Topic'\n",
    "    \n",
    "    # Merge frequency/temporal stats with DTM metrics\n",
    "    integrated_stats = pd.merge(topic_stats, dtm_metrics, on='Topic', how='left').fillna(0)\n",
    "    integrated_stats['Duration_Days'] = (integrated_stats['End_Date'] - integrated_stats['Start_Date']).dt.days\n",
    "    \n",
    "    # Sort by a combined score or frequency\n",
    "    integrated_stats = integrated_stats.sort_values('Frequency_sum', ascending=False)\n",
    "    \n",
    "    print(f\"🏆 Top 10 Topics by Frequency (with TTQ Score):\")\n",
    "    print(integrated_stats.head(10).round(3))\n",
    "    return integrated_stats\n",
    "\n",
    "def create_integrated_topic_charts(df, dataset_name):\n",
    "    \"\"\"Create charts from the integrated topic statistics DataFrame.\"\"\"\n",
    "    if df is None: return\n",
    "    \n",
    "    print(f\"\\n📈 Creating integrated charts for {dataset_name}\")\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2, \n",
    "        subplot_titles=(\n",
    "            'Total Frequency vs. TTQ Score',\n",
    "            'Top 20 Topics by TTQ Score',\n",
    "            'Topic Duration vs. TTQ Score',\n",
    "            'Topic Persistence (Time Periods) vs. TTQ Score'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Scatter plot: Frequency vs. TTQ\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df['Frequency_sum'], \n",
    "        y=df['TTQ_Score'], \n",
    "        mode='markers', \n",
    "        marker=dict(size=10, color=df['Time_Periods'], colorscale='Viridis', showscale=True, colorbar=dict(title='Time Periods')),\n",
    "        text=df['Topic'].apply(lambda x: f'Topic {x}')\n",
    "    ), row=1, col=1)\n",
    "\n",
    "    # Bar chart: Top topics by TTQ\n",
    "    top_ttq = df.nlargest(20, 'TTQ_Score')\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=top_ttq['Topic'].astype(str), \n",
    "        y=top_ttq['TTQ_Score'],\n",
    "        marker_color=top_ttq['Frequency_sum']\n",
    "    ), row=1, col=2)\n",
    "\n",
    "    # Scatter plot: Duration vs TTQ\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df['Duration_Days'], \n",
    "        y=df['TTQ_Score'], \n",
    "        mode='markers',\n",
    "        marker=dict(color=df['Frequency_sum'], colorscale='Plasma', showscale=True, colorbar=dict(title='Frequency'))\n",
    "    ), row=2, col=1)\n",
    "\n",
    "    # Scatter plot: Time Periods vs TTQ\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df['Time_Periods'], \n",
    "        y=df['TTQ_Score'], \n",
    "        mode='markers',\n",
    "        marker=dict(color=df['Frequency_sum'], colorscale='Inferno', showscale=True, colorbar=dict(title='Frequency'))\n",
    "    ), row=2, col=2)\n",
    "\n",
    "    fig.update_layout(height=800, width=1200, title_text=f'Integrated Topic Analysis for {dataset_name}', showlegend=False)\n",
    "    fig.update_xaxes(title_text=\"Total Frequency\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"TTQ Score\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Topic ID\", type='category', row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"TTQ Score\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Duration (Days)\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"TTQ Score\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Number of Time Periods\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"TTQ Score\", row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Run integrated analysis\n",
    "youtube_integrated_stats = get_all_topics_integrated_overview(youtube_data, youtube_calculator)\n",
    "create_integrated_topic_charts(youtube_integrated_stats, \"YouTube\")\n",
    "\n",
    "telegram_integrated_stats = get_all_topics_integrated_overview(telegram_data, telegram_calculator)\n",
    "create_integrated_topic_charts(telegram_integrated_stats, \"Telegram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and How to Use\n",
    "\n",
    "This notebook provides a complete analysis pipeline, from raw data to integrated topic profiles.\n",
    "\n",
    "### How to Access the Integrated Data\n",
    "\n",
    "The DataFrames `youtube_integrated_stats` and `telegram_integrated_stats` contain all the information for each topic. You can use this for further custom analysis.\n",
    "\n",
    "```python\n",
    "# Example: View the top 5 YouTube topics with all their stats\n",
    "print(youtube_integrated_stats.head())\n",
    "\n",
    "# Example: Find the topic with the highest TTQ score\n",
    "best_topic = youtube_integrated_stats.loc[youtube_integrated_stats['TTQ_Score'].idxmax()]\n",
    "print(\"\\nTopic with the best TTQ Score:\")\n",
    "print(best_topic)\n",
    "```\n",
    "\n",
    "### Key Insights from Integrated Analysis\n",
    "\n",
    "By combining frequency metrics with quality metrics like TTQ, you can answer deeper questions:\n",
    "- Are my most frequent topics also high-quality and coherent over time?\n",
    "- Are there any high-quality 'hidden gem' topics that are not very frequent but are very consistent?\n",
    "- Do long-lasting topics tend to have higher or lower quality scores?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
